{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "cBeJkN0pT2KR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQABlLVHStW0"
      },
      "outputs": [],
      "source": [
        " import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Bidirectional, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from keras.regularizers import l2\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read File"
      ],
      "metadata": {
        "id": "gLY1T_WeT4Kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('your file name')"
      ],
      "metadata": {
        "id": "l3j5LRRjTdKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "xIyjCkK3T69o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, SpatialDropout1D, BatchNormalization, GlobalMaxPooling1D, Conv1D, MaxPooling1D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Define your feature (X) and target (y)\n",
        "X = df[' Your X column values']\n",
        "y = [labels.split(', ') for labels in df['Your Y column values']]  # Assumes that labels are separated by ', '\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a MultiLabelBinarizer\n",
        "mlb1 = MultiLabelBinarizer()\n",
        "\n",
        "# Transform the target labels into a binary format\n",
        "y_train_bin = mlb1.fit_transform(y_train)\n",
        "y_test_bin = mlb1.transform(y_test)\n",
        "\n",
        "# Tokenize the text data\n",
        "max_words = 10000  # You can adjust this value\n",
        "tokenizer1 = Tokenizer(num_words=max_words)\n",
        "tokenizer1.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer1.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer1.texts_to_sequences(X_test)\n",
        "\n",
        "max_sequence_length = 200  # You can adjust this value\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
        "\n",
        "# Create a more complex deep learning model\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(max_words, 256, input_length=max_sequence_length))\n",
        "model1.add(SpatialDropout1D(0.2))  # Spatial dropout for embeddings\n",
        "model1.add(Conv1D(128, 5, activation='relu'))  # Convolutional layer\n",
        "model1.add(MaxPooling1D(5))  # Max-pooling layer\n",
        "model1.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model1.add(SpatialDropout1D(0.2))\n",
        "model1.add(Conv1D(64, 5, activation='relu'))  # Additional convolutional layer\n",
        "model1.add(MaxPooling1D(5))\n",
        "model1.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model1.add(GlobalMaxPooling1D())  # Pooling layer for sequence data\n",
        "model1.add(Dense(256, activation='relu'))\n",
        "model1.add(BatchNormalization())  # Batch normalization for improved training stability\n",
        "model1.add(Dropout(0.5))\n",
        "model1.add(Dense(len(mlb1.classes_), activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.005)\n",
        "model1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model1.fit(X_train_padded, y_train_bin, epochs=300, batch_size=64)\n",
        "\n",
        "# Make multi-label predictions on the test set\n",
        "y_pred_bin = model1.predict(X_test_padded)\n",
        "\n",
        "# Apply a threshold to convert probabilities to binary labels\n",
        "threshold = 0.7\n",
        "y_pred_bin = (y_pred_bin > threshold).astype(int)\n",
        "\n",
        "# Calculate accuracy and provide a classification report\n",
        "accuracy = accuracy_score(y_test_bin, y_pred_bin)\n",
        "classification_rep = classification_report(y_test_bin, y_pred_bin)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test_bin, y_pred_bin, average='weighted')\n",
        "recall = recall_score(y_test_bin, y_pred_bin, average='weighted')\n",
        "f1 = f1_score(y_test_bin, y_pred_bin, average='weighted')\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "\n",
        "\n",
        "# Print the classification report\n",
        "classification_rep = classification_report(y_test_bin, y_pred_bin, target_names=mlb1.classes_)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)\n"
      ],
      "metadata": {
        "id": "WxIT5q1hTdIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "-FtWMgpuT9ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1.save(\"model5.h5\")\n",
        "\n",
        "import pickle\n",
        "with open(\"tokenizer.pkl\", \"wb\") as tokenizer_file:\n",
        "    pickle.dump(tokenizer1, tokenizer_file)\n",
        "\n",
        "\n",
        "# Save MultiLabelBinarizer\n",
        "joblib.dump(mlb1, \"mlb5.pkl\")"
      ],
      "metadata": {
        "id": "2XFvTanPTdFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the model"
      ],
      "metadata": {
        "id": "Utpv5bFMUAYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rec\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import model_from_json\n",
        "import json\n",
        "import joblib\n",
        "from keras.models import load_model\n",
        "# Load the saved model\n",
        "\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = load_model(\"model5.h5\")\n",
        "\n",
        "with open(\"tokenizer.pkl\", \"rb\") as tokenizer_file:\n",
        "    loaded_tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "# Load MultiLabelBinarizer (if needed)\n",
        "loaded_mlb = joblib.load(\"mlb5.pkl\")\n",
        "\n",
        "# Function for preprocessing text\n",
        "def text_preprocessing(text):\n",
        "    # Remove non-ASCII characters\n",
        "    text = ''.join([c if ord(c) < 128 else ' ' for c in text])\n",
        "\n",
        "    # Remove Roman numerals using a regular expression\n",
        "    text = re.sub(r'\\b[IVXLCDM]+(?:\\s+[ivxlcdm]+)?\\b', '', text)  # Remove lowercase Roman numerals\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    text = text.replace('“', '')\n",
        "    text = text.replace('”', '')\n",
        "    text = text.replace('’', '')\n",
        "    text = text.replace('\\n', ' ').replace('-', ' ')\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub(r',+', ',  ,', text)\n",
        "\n",
        "    # Tokenize the text and remove stopwords\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Function to predict labels for the provided text\n",
        "def predict_labels_for_text(user_text, threshold=0.7):\n",
        "\n",
        "    # Preprocess the input text\n",
        "    preprocessed_text = text_preprocessing(user_text)\n",
        "    text_sequence = loaded_tokenizer.texts_to_sequences([preprocessed_text])\n",
        "    max_sequence_length = 200  # You can adjust this value\n",
        "    padded_sequence = pad_sequences(text_sequence, maxlen=max_sequence_length)\n",
        "\n",
        "    # Predict labels for the text\n",
        "    text_labels_bin = loaded_model.predict(padded_sequence)\n",
        "    text_labels = loaded_mlb.inverse_transform(text_labels_bin > threshold)\n",
        "\n",
        "    return text_labels\n",
        "\n",
        "user_input_text = input(\"Enter text: \\n\")\n",
        "\n",
        "# Predict labels for the user-provided text\n",
        "predicted_labels = predict_labels_for_text(user_input_text)\n",
        "\n",
        "# Extract label strings from tuples and display the predicted labels\n",
        "predicted_label_strings = [', '.join(label_tuple) for label_tuple in predicted_labels]\n",
        "print(f\"Predicted Labels: {', '.join(predicted_label_strings)}\")\n"
      ],
      "metadata": {
        "id": "KzApyTkUTdDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Cgf4ZfRTdAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CXBSqjs8Tc-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5xyCXouTc76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D9UvB_BATc5p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}